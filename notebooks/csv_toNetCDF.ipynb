{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NetCDF from .csv file\n",
    "\n",
    "Experiement to use metsim (python pacage) to convert HADS gage time series data into NetCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge cartopy --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge geoviews --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge metsim --y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "begin with some standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update matplotlib --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import cartopy\n",
    "import geoviews as gv\n",
    "import geopandas as gpd\n",
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from metsim import MetSim\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParas['figure.dpi'] = 96\n",
    "hv.notebook_extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together the required meteorological data\n",
    "We've gathered some data form the HADS site that we will use as input. There are different sets of CSVs with precipitation, with data for 2009 and 2010, and will be generating MetSim input for 2010. To do this we must first convert it into an xarray dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HADS example files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datameta = pd.read_csv(\"../example_data/HADS/DCPbySTATE_HI_station_info.csv\")\n",
    "data = pd.read_csv(\"../example_data/HADS/HADS_20190323.txt\", sep=\"|\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datameta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"nesdis_id\", \"nwsli\", \"type\",\"datetime\", \"cumP\", \"5\",\"6\"]\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1560029C', '156011EA'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nesdis_id.unique()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    latitude_d  longitude_d\n",
      "6      22.1375     -159.312\n",
      "25     21.9364     -159.587\n"
     ]
    }
   ],
   "source": [
    "latlongs=pd.DataFrame()\n",
    "for i in data.nesdis_id.unique()[0:2]:\n",
    "    temp=datameta.loc[datameta['nesdis_id'] == i, 'latitude_d':'longitude_d']\n",
    "    latlongs=latlongs.append(temp)\n",
    "    \n",
    "print(latlongs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not quite working yet\n",
    "error message:\n",
    "1560029C\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-20-91d6b020cfe3> in <module>()\n",
    "     57                 threehrP=sumeveryxrows(temp,3)\n",
    "     58                 threehrP=np.append(threehrP,sum(hrP[-mod:]))\n",
    "---> 59         threehrP=np.append(sum(temp1),threehrP)\n",
    "     60     #print(threehrP)\n",
    "\n",
    "TypeError: 'numpy.float64' object is not iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560029C\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-91d6b020cfe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mthreehrP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msumeveryxrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mthreehrP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreehrP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhrP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mthreehrP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreehrP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;31m#print(threehrP)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "sumeveryxrows = lambda myarray, x: myarray.values.reshape((myarray.shape[0] // x, x)).sum(1)\n",
    "from datetime import datetime\n",
    "for i in data.nesdis_id.unique()[0:2]:\n",
    "    print(i)\n",
    "    #i=data.nesdis_id.unique()[0]\n",
    "    sitedata = data.loc[(data.nesdis_id == i),:]\n",
    "    cumP_tnp1= np.append([np.nan],sitedata.cumP)\n",
    "    unitP=sitedata.cumP-cumP_tnp1[0:-1]\n",
    "    sitedata = pd.concat([sitedata, unitP], axis=1)\n",
    "    #sitedata.columns = ['unitP']\n",
    "    sitedata.columns=['nesdis_id','nwsli','type','datetime','cumP','5','6','unitP']\n",
    "    #print(sitedata)\n",
    "    # Create a pandas series, using 'time' as the index\n",
    "    #print(sitedata.unitP)\n",
    "    #print(sitedata.datetime.to_string())\n",
    "    datetime_list = [datetime.strptime(time, '%Y-%m-%d %H:%M') for time in sitedata.datetime.astype(str)]\n",
    "    #print(datetime_list)\n",
    "    #datetime_object = datetime.strptime((sitedata.datetime.astype(str)[0]), '%Y-%m-%d %H:%M')\n",
    "    #datetime.strptime(\"21/11/06 16:30\", \"%d/%m/%y %H:%M\")\n",
    "    series = pd.Series(np.array(sitedata.unitP), index=datetime_list)\n",
    "    hrP=series.groupby([series.index.date, series.index.hour]).sum()\n",
    "    #print(hrP) \n",
    "    ## turn it into every 3 hour\n",
    "    ## WRF-Hydro run by UTC0, Hawaii is (23-1,2-4,5-7,8-10,11-13,14-16,17-19,20-22)hr\n",
    "    firsthr=hrP.index[0][1]\n",
    "    if firsthr==23|firsthr==2|firsthr==5|firsthr==8|firsthr==11|firsthr==14|firsthr==17|firsthr==20:\n",
    "        if len(hrP)%3==0:\n",
    "            threehrP=sumeveryxrows(hrP,3)\n",
    "        else:\n",
    "            mod=len(hrP)%3\n",
    "            temp=hrP[0:-mod]\n",
    "            threehrP=sumeveryxrows(temp,3)\n",
    "            threehrP=np.append(threehrP,sum(hrP[-mod:]))\n",
    "    elif firsthr==0|firsthr==3|firsthr==4|firsthr==5|firsthr==12|firsthr==15|firsthr==18|firsthr==21: #need to revise later\n",
    "        temp1=hrP[0:2]\n",
    "        hrP=hrP[2:]\n",
    "        firsthr=hrP.index[0][1]\n",
    "        if firsthr==23|firsthr==2|firsthr==5|firsthr==8|firsthr==11|firsthr==14|firsthr==17|firsthr==20:\n",
    "            if len(hrP)%3==0:\n",
    "                    threehrP=sumeveryxrows(hrP,3)\n",
    "            else:\n",
    "                mod=len(hrP)%3\n",
    "                temp=hrP[0:-mod]\n",
    "                threehrP=sumeveryxrows(temp,3)\n",
    "                threehrP=np.append(threehrP,sum(hrP[-mod:]))\n",
    "        threehrP=np.append(sum(temp1),threehrP)\n",
    "    else:\n",
    "        temp1=hrP[0]\n",
    "        hrP=hrP[1:]\n",
    "        firsthr=hrP.index[0][1]\n",
    "        if firsthr==23|firsthr==2|firsthr==5|firsthr==8|firsthr==11|firsthr==14|firsthr==17|firsthr==20:\n",
    "            if len(hrP)%3==0:\n",
    "                    threehrP=sumeveryxrows(hrP,3)\n",
    "            else:\n",
    "                mod=len(hrP)%3\n",
    "                temp=hrP[0:-mod]\n",
    "                threehrP=sumeveryxrows(temp,3)\n",
    "                threehrP=np.append(threehrP,sum(hrP[-mod:]))\n",
    "        threehrP=np.append(sum(temp1),threehrP)\n",
    "    #print(threehrP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the dataset with the relevant dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('3/17/2019', '3/23/2019')\n",
    "shape = (len(dates), 1, 1, )\n",
    "dims = ('time', 'lat', 'lon', )\n",
    "\n",
    "# We are running only one site, at these coordinates\n",
    "lats = [19.8372, 21.4319]\n",
    "lons = [-155.613, -157.837]\n",
    "#elev = 1706.90 # meters\n",
    "coords = {'time': dates, 'lat': lats, 'lon': lons}\n",
    "\n",
    "# Create the initial met data input data structure\n",
    "met_data = xr.Dataset(coords=coords)\n",
    "met_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the acutal data arrays to put data into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for varname in ['prec']:\n",
    "    met_data[varname] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                     coords=coords, dims=dims,\n",
    "                                     name=varname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and put it into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lats)):\n",
    "    for j in range(len(lons)):\n",
    "        # Add the precipitation data\n",
    "        df = pd.read_csv(\"./reynolds_creek_data/daily/precip_2010.csv\", skiprows=[0,1])\n",
    "        prec_vals = df[].diff().values[1:]*25.4\n",
    "        met_data['prec'].values[:, i, j] = prec_vals\n",
    "\n",
    "# This is what we have now\n",
    "met_data.to_netcdf('./input/rc_forcing.nc')\n",
    "met_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together the required domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We form the domain in a similar fashion\n",
    "# First, by creating the data structure\n",
    "coords = {'lat': lats, 'lon': lons}\n",
    "domain = xr.Dataset(coords=coords)\n",
    "domain['elev'] = xr.DataArray(data=np.full((1,1,), np.nan),\n",
    "                          coords=coords,\n",
    "                          dims=('lat', 'lon', ))\n",
    "domain['mask'] = xr.DataArray(data=np.full((1,1,), np.nan),\n",
    "                          coords=coords,\n",
    "                          dims=('lat', 'lon', ))\n",
    "\n",
    "# Add the data\n",
    "domain['elev'][0, 0] = elev\n",
    "domain['mask'][0, 0] = 1\n",
    "domain.to_netcdf('./input/rc_domain.nc')\n",
    "domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together the required state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create the state file - the dates are 90 days prior to \n",
    "# the MetSim run dates - as usual, create an empty data structure to\n",
    "# read the data into\n",
    "dates = pd.date_range('3/16/2019', '3/23/2019')\n",
    "shape = (len(dates), 1, 1, )\n",
    "dims = ('time', 'lat', 'lon', )\n",
    "\n",
    "coords = {'time': dates, 'lat': lats, 'lon': lons}\n",
    "state = xr.Dataset(coords=coords)\n",
    "for varname in ['prec', 't_min', 't_max']:\n",
    "    state[varname] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                               coords=coords, dims=dims,\n",
    "                               name=varname)\n",
    "\n",
    "# Do precip data\n",
    "df = pd.read_csv(\"./reynolds_creek_data/daily/precip_2009.csv\", skiprows=[0,1])\n",
    "prec_vals = df['PREC.I-1 (in) '].diff().values[-90:] * 25.4\n",
    "state['prec'].values[:, 0, 0] = prec_vals\n",
    "\n",
    "# And now temp data\n",
    "df = pd.read_csv(\"./reynolds_creek_data/daily/temp_2009.csv\", skiprows=[0,1])\n",
    "tmin_vals = df['TMIN.D-1 (degC) '].values[-90:]\n",
    "tmax_vals = df['TMAX.D-1 (degC) '].values[-90:]\n",
    "state['t_min'].values[:, 0, 0] = tmin_vals\n",
    "state['t_max'].values[:, 0, 0] = tmax_vals\n",
    "state.to_netcdf('./input/rc_state.nc')\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering parameters and building the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('1/1/2010', '12/31/2010')\n",
    "params = {\n",
    "    'time_step'    : \"60\",       \n",
    "    'start'        : dates[0],\n",
    "    'stop'         : dates[-1],\n",
    "    'forcing'      : './input/rc_forcing.nc',     \n",
    "    'domain'       : './input/rc_domain.nc',\n",
    "    'state'        : './input/rc_state.nc',\n",
    "    'forcing_fmt'  : 'netcdf',\n",
    "    'out_dir'      : './output',\n",
    "    'output_prefix': 'reynolds',\n",
    "    'scheduler'    : 'threading',\n",
    "    'chunks'       : \n",
    "        {'lat': 1, 'lon': 1},\n",
    "    'forcing_vars' : \n",
    "        {'prec' : 'prec', 't_max': 't_max', 't_min': 't_min'},\n",
    "    'state_vars'   : \n",
    "        {'prec' : 'prec', 't_max': 't_max', 't_min': 't_min'},\n",
    "    'domain_vars'  : \n",
    "        {'elev': 'elev', 'lat': 'lat', 'lon': 'lon', 'mask': 'mask'}\n",
    "    }               \n",
    "\n",
    "ms = MetSim(params)\n",
    "ms.run()\n",
    "output = ms.open_output().load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
